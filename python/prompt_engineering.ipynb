{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = os.getenv(\"RUNPOD_TOKEN\"),\n",
    "    base_url = os.getenv(\"RUNPOD_BASE_URL\")\n",
    ")\n",
    "model_name = os.getenv(\"MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Why is the capital of Egypt?\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get llama response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatbot_response(client, model_name, messages, temperature=0):\n",
    "    input_messages = []\n",
    "    for message in messages:\n",
    "        input_messages.append({\"role\": message[\"role\"], \"content\": message[\"content\"]})\n",
    "    # Create a chat completion\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=input_messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chatbot_response(client, model_name, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "## Structured output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helper assistant that answer questions about crispr cas9.\n",
    "Your output should be in a structured json format exactly like the one the below. You are not allowed to write anything other than json object:\n",
    "[\n",
    "    {\n",
    "        \"question\": the question that you will answer\n",
    "        \"answer\": concise answer for the question\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt}\n",
    "]\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": \"How to design a crispr-cas9 experiment for gene editing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_chatbot_response(client, model_name, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'How to design a CRISPR-Cas9 experiment for gene editing?', 'answer': {'steps': ['1. Identify the target gene and its function', '2. Design a guide RNA (gRNA) that specifically binds to the target gene', '3. Choose a suitable Cas9 enzyme and its variant (e.g., SpCas9, AsCas9)', '4. Optimize the gRNA-Cas9 complex design for efficient editing', '5. Select a suitable cell type or organism for editing', '6. Validate the gRNA-Cas9 complex using in vitro assays (e.g., PCR, sequencing)', '7. Perform the CRISPR-Cas9 editing experiment using the optimized gRNA-Cas9 complex', '8. Verify the editing efficiency and specificity using molecular biology techniques (e.g., PCR, sequencing, Western blot)', '9. Analyze the edited cells or organisms for desired phenotypes or functions']}}]\n"
     ]
    }
   ],
   "source": [
    "json_response = json.loads(response)\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"\"\"\n",
    "Get me basic information on different crispr systems:\n",
    "    ```\n",
    "        1. cas9\n",
    "        2. cas12a\n",
    "        3. prime editor\n",
    "        4. base editor\n",
    "    ```\n",
    "\"\"\"\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt}\n",
    "]\n",
    "messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "response = get_chatbot_response(client, model_name, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = json.loads(response)\n",
    "print(json.dumps(json_response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give the model time to think (Chain of thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "    calculate the result of this equation: 1 + 3\n",
    "    Your output should be in a structured json format exactly like below. You are not allowed to write anything else other than json object.\n",
    "\n",
    "    {\n",
    "        result: the final number resulted from calculating the equation above.\n",
    "    \n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"result\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"steps\": \"First, we calculate the division: 259 / 2 = 129.5. Then, we multiply 129.5 by 8654: 129.5 * 8654 = 112,141.5. Next, we multiply 91072 by 33: 91072 * 33 = 3,003,336. Finally, we add 112,141.5 and 3,003,336, then subtract 12971: 112,141.5 + 3,003,336 = 3,115,477.5, and 3,115,477.5 - 12971 = 3,106,506.5.\",\n",
      "    \"result\": 3106506.5\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "    calculate the result of this equation: 259 / 2 * 8654 + 91072 * 33 - 12971 \n",
    "    Your output should be in a structured json format exactly like below. You are not allowed to write anything else other than json object.\n",
    "\n",
    "    {\n",
    "        steps: This is where you solve the equation step by step following BEDMAS order of operation. You need to show your work and calculate each step leading to the final result. Feel free to write in free text.\n",
    "        result: the final number resulted from calculating the equation above.\n",
    "    \n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Retrival Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "    what is new about primer editors?\n",
    "\"\"\"\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_16 = \"\"\"\n",
    "The iPhone 16 introduces several exciting updates, making it one of Apple's most advanced smartphones to date. It features a larger 6.1-inch display for the base model and a 6.7-inch screen for the iPhone 16 Plus, with thinner bezels and a more durable Ceramic Shield. The iPhone 16 Pro and Pro Max boast even larger displays, measuring 6.3 and 6.9 inches respectively, offering the thinnest bezels seen on any Apple product so far.\n",
    "\n",
    "Powered by the new A18 chip (A18 Pro for the Pro models), these phones deliver significant performance improvements, with enhanced neural engine capabilities, faster GPU for gaming, and machine learning tasks. The camera systems are also upgraded, with the base iPhone 16 sporting a dual-camera setup with a 48MP main sensor. The Pro models offer a 48MP Ultra Wide and 5x telephoto camera, enhanced by Appleâ€™s \"Camera Control\" button for more flexible photography options.\n",
    "\n",
    "Apple also introduced advanced audio features like \"Audio Mix,\" which uses machine learning to separate background sounds from speech, allowing for more refined audio capture during video recording. Battery life has been extended, especially in the iPhone 16 Pro Max, which is claimed to have the longest-lasting battery of any iPhone \n",
    "9TO5MAC\n",
    "\n",
    "APPLEMAGAZINE\n",
    ".\n",
    "\n",
    "Additionally, Apple has switched to USB-C for faster charging and data transfer, and the Pro models now support up to 2x faster video encoding. The starting prices remain consistent with previous generations, with the iPhone 16 starting at $799, while the Pro models start at $999\n",
    "\"\"\"\n",
    "\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic extraction of context from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "samsung_s23 = \"\"\"\n",
    "The Samsung Galaxy S23 brings some incremental but notable upgrades to its predecessor, the Galaxy S22. It features the Snapdragon 8 Gen 2 processor, a powerful chip optimized for the S23 series, delivering enhanced performance, especially for gaming and multitasking. This chip ensures top-tier speed and efficiency across all models, from the base S23 to the larger S23+ and S23 Ultraâ€‹\n",
    "STUFF\n",
    "\n",
    "TECHRADAR\n",
    ".\n",
    "\n",
    "In terms of design, the S23's camera module has been streamlined by removing the raised metal contour around the cameras, creating a cleaner, sleeker look. It also sports the same 6.1-inch 120Hz AMOLED display, protected by tougher Gorilla Glass Victus 2, making it more resistant to scratches and dropsâ€‹\n",
    "TECHRADAR\n",
    ".\n",
    "\n",
    "The S23 Ultra stands out with its 200MP main camera, offering impressive photo clarity, especially in low-light conditions. The selfie camera across the series has been updated to a 12MP sensor, resulting in sharper selfies. The Ultra model also includes productivity tools such as the S-Pen, which remains an essential feature for note-taking and creative tasksâ€‹\n",
    "STUFF\n",
    "\n",
    "TECHRADAR\n",
    ".\n",
    "\n",
    "Battery life is solid, with the S23 Ultra featuring a 5000mAh battery that lasts comfortably through a day of heavy use. However, charging speeds still lag behind some competitors, with 45W wired charging, which is slower than other brands offering up to 125W chargingâ€‹\n",
    "STUFF\n",
    ".\n",
    "\n",
    "Overall, the Galaxy S23 series enhances performance, durability, and camera quality, making it a strong contender for users seeking a high-performance flagship.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [iphone_16, samsung_s23]\n",
    "user_prompt = \"\"\"\n",
    "    What is new in samsung a10?\n",
    "\"\"\"\n",
    "embedding_client = OpenAI(\n",
    "    api_key = os.getenv(\"RUNPOD_TOKEN\"),\n",
    "    base_url = os.getenv(\"RUNPOD_EMBEDDING_URL\")\n",
    ")\n",
    "\n",
    "embedding_model_name = os.getenv(\"EMBEDDING_MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(embedding_client, model_name, text_input):\n",
    "    output = embedding_client.embeddings.create(\n",
    "    input=text_input,\n",
    "    model=model_name\n",
    ")\n",
    "    \n",
    "    embeddings = []\n",
    "    for embedding in output.data:\n",
    "        embeddings.append(embedding.embedding)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_embedding(embedding_client, embedding_model_name, user_prompt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embedding = [get_embedding(embedding_client, embedding_model_name, text)[0] for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_similarity = cosine_similarity([output], data_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74482716, 0.52195224]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
